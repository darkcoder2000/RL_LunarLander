{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install \"ray[rllib]\"\n",
    "#!pip install \"ray[tune]\"\n",
    "#!pip install torch==1.10.2+cu113 torchvision==0.11.3+cu113 torchaudio===0.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "#!pip install redis\n",
    "\n",
    "#!pip install Box2D\n",
    "#!pip install box2d-py\n",
    "#!pip install gym[all]\n",
    "#!pip install gym[Box_2D]\n",
    "\n",
    "import gym\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "all_envs = envs.registry.all()\n",
    "env_ids = [env_spec.id for env_spec in all_envs]\n",
    "print(sorted(env_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Tetris-v5')\n",
    "sample = env.observation_space.sample()\n",
    "print(sample)\n",
    "print(sample.shape)\n",
    "action = env.action_space.sample()\n",
    "print(action)\n",
    "print(action.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Tetris-v5',render_mode='human')\n",
    "\n",
    "env.reset()\n",
    "#env.render()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    print(action)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(reward)\n",
    "    #env.render()\n",
    "\n",
    "    if done:\n",
    "        env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the algorithm.\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"LunarLander-v2\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 4,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"torch\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [128, 128, 128, 128],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": True,\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create our RLlib Trainer.\n",
    "trainer = PPOTrainer(config=config)\n",
    "\n",
    "# Run it for n training iterations. A training iteration includes\n",
    "# parallel sample collection by the environment workers as well as\n",
    "# loss calculation on the collected batch and a model update.\n",
    "for _ in range(10):\n",
    "    print(trainer.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained Trainer (and render each timestep to the shell's\n",
    "# output).\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "# How many time steps to run the experiment for.\n",
    "time_steps_total = 1_000_000\n",
    "\n",
    "# Where the trained agents and the logs will end up.\n",
    "local_dir = \"tune_runs_lunarlander\"\n",
    "\n",
    "# Run the experiment.\n",
    "results = tune.run(\n",
    "    agents.ppo.PPOTrainer,\n",
    "    config=config,\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    stop={\"timesteps_total\": time_steps_total},\n",
    "    checkpoint_at_end=True,\n",
    "    checkpoint_freq=10,\n",
    "    # keep_checkpoints_num=10,\n",
    "    local_dir=local_dir,\n",
    "    #restore=restore_checkpoint\n",
    "    #        name=\"PPOTrainer_2022-03-23_15-47-45\",\n",
    "    #resume=AUTO\n",
    ")\n",
    "\n",
    "# Get the checkpoints.\n",
    "checkpoints = results.get_trial_checkpoints_paths(\n",
    "    trial=results.get_best_trial(\"episode_reward_mean\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "for checkpoint in checkpoints:\n",
    "    checkpoint_path = checkpoint[0]\n",
    "    print(\"Checkpoint path:\", checkpoint_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Clean up.\n",
    "subfolders = glob.glob(os.path.join(local_dir, \"*\"), recursive=True)\n",
    "subfolders = [subfolder for subfolder in subfolders if os.path.isdir(subfolder)]\n",
    "for subfolder in subfolders:\n",
    "    subsubfolders = glob.glob(os.path.join(subfolder, \"*\"), recursive=True)\n",
    "    subsubfolders = [subsubfolder for subsubfolder in subsubfolders if os.path.isdir(subsubfolder)]\n",
    "    for subsubfolder in subsubfolders:\n",
    "        subsubfolder_cleaned = subsubfolder.replace(\"[\", \"\").replace(\"]\", \"\").replace(\", \", \"-\")\n",
    "        if subsubfolder_cleaned != subsubfolder:\n",
    "            os.rename(subsubfolder, subsubfolder_cleaned)\n",
    "            print(f\"Renamed {subsubfolder} to {subsubfolder_cleaned}\")\n",
    "\n",
    "# Sound an alarm using unix escape sequence.\n",
    "print(\"Done.\")\n",
    "print(\"\\a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib import agents\n",
    "# Where the trained agents and the logs will end up.\n",
    "local_dir = \"tune_runs_lunarlander\"\n",
    "\n",
    "# Find all the occurences of params.json in the directory tune_runs.\n",
    "params_paths = glob.glob(os.path.join(local_dir, \"**\", \"params.json\"), recursive=True)\n",
    "params_paths = sorted(params_paths)\n",
    "\n",
    "# Find all the checkpoints per params.json.\n",
    "pairs = []\n",
    "for params_path in params_paths:\n",
    "    search_path = os.path.join(os.path.dirname(params_path), \"**\")\n",
    "    checkpoint_paths = glob.glob(search_path, recursive=True)\n",
    "    checkpoint_paths = [checkpoint_path for checkpoint_path in checkpoint_paths if\n",
    "                        not os.path.isdir(checkpoint_path)]\n",
    "    checkpoint_paths = [checkpoint_path for checkpoint_path in checkpoint_paths if\n",
    "                        os.path.basename(checkpoint_path).startswith(\n",
    "                            \"checkpoint-\") and not checkpoint_path.endswith(\".tune_metadata\")]\n",
    "    checkpoint_paths = sorted(checkpoint_paths)\n",
    "    pairs += [(params_path, checkpoint_path) for checkpoint_path in checkpoint_paths]\n",
    "\n",
    "# Get the user input.\n",
    "if len(pairs) == 0:\n",
    "    print(\"No checkpoints found.\")\n",
    "elif len(pairs) == 1:\n",
    "    user_index = 0\n",
    "else:\n",
    "    print(\"Select a checkpoint:\")\n",
    "    for index, (_, subfolder) in enumerate(pairs):\n",
    "        print(f\"{index: >2}: {subfolder}\")\n",
    "    user_index = int(input(\"Enter the index of the checkpoint: \"))\n",
    "\n",
    "# Get config path and checkpoint path.\n",
    "config_path, checkpoint_path = pairs[user_index]\n",
    "assert os.path.exists(config_path)\n",
    "assert os.path.exists(checkpoint_path)\n",
    "print(\"Config path:\", config_path)\n",
    "print(\"Checkpoint path:\", checkpoint_path)\n",
    "\n",
    "# Read the config.\n",
    "with open(config_path, \"r\") as file:\n",
    "    enjoy_config = json.load(file)\n",
    "    enjoy_config = {key: value for key, value in enjoy_config.items() if key not in [\"num_gpus\", \"num_workers\"]}\n",
    "\n",
    "print(enjoy_config, config_path)\n",
    "\n",
    "# Load the agent.\n",
    "print(\"Loading agent...\")\n",
    "agent = agents.ppo.PPOTrainer(config=enjoy_config)\n",
    "agent.restore(checkpoint_path)\n",
    "print(\"Agent loaded.\")\n",
    "\n",
    "# Create the environment.\n",
    "print(\"Creating the environment.\")\n",
    "environment = gym.make('LunarLander-v2')\n",
    "observation = environment.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    #print(observation[\"carriers_next_station_distance\"])\n",
    "    action = agent.compute_action(observation)\n",
    "        \n",
    "    # Execute the action.\n",
    "    observation, reward, done, info = environment.step(action)\n",
    "    \n",
    "    # Render the environment and print the data.\n",
    "    environment.render()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed07d4edd1120f0c6f7c45af132c922a7b7ce032b0fa0a94e546d7e5ab69163c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('RL_GymStableRay')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
