{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install \"ray[rllib]\"\n",
    "#!pip install \"ray[tune]\"\n",
    "#!pip install torch==1.10.2+cu113 torchvision==0.11.3+cu113 torchaudio===0.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "#!pip install redis\n",
    "\n",
    "#!pip install Box2D\n",
    "#!pip install box2d-py\n",
    "#!pip install gym[all]\n",
    "#!pip install gym[Box_2D]\n",
    "\n",
    "import gym\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Acrobot-v1', 'Ant-v2', 'Ant-v3', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'Blackjack-v1', 'CarRacing-v0', 'CartPole-v0', 'CartPole-v1', 'CliffWalking-v0', 'CubeCrash-v0', 'CubeCrashScreenBecomesBlack-v0', 'CubeCrashSparse-v0', 'FetchPickAndPlace-v1', 'FetchPickAndPlaceDense-v1', 'FetchPush-v1', 'FetchPushDense-v1', 'FetchReach-v1', 'FetchReachDense-v1', 'FetchSlide-v1', 'FetchSlideDense-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HandManipulateBlock-v0', 'HandManipulateBlockDense-v0', 'HandManipulateBlockFull-v0', 'HandManipulateBlockFullDense-v0', 'HandManipulateBlockRotateParallel-v0', 'HandManipulateBlockRotateParallelDense-v0', 'HandManipulateBlockRotateParallelTouchSensors-v0', 'HandManipulateBlockRotateParallelTouchSensors-v1', 'HandManipulateBlockRotateParallelTouchSensorsDense-v0', 'HandManipulateBlockRotateParallelTouchSensorsDense-v1', 'HandManipulateBlockRotateXYZ-v0', 'HandManipulateBlockRotateXYZDense-v0', 'HandManipulateBlockRotateXYZTouchSensors-v0', 'HandManipulateBlockRotateXYZTouchSensors-v1', 'HandManipulateBlockRotateXYZTouchSensorsDense-v0', 'HandManipulateBlockRotateXYZTouchSensorsDense-v1', 'HandManipulateBlockRotateZ-v0', 'HandManipulateBlockRotateZDense-v0', 'HandManipulateBlockRotateZTouchSensors-v0', 'HandManipulateBlockRotateZTouchSensors-v1', 'HandManipulateBlockRotateZTouchSensorsDense-v0', 'HandManipulateBlockRotateZTouchSensorsDense-v1', 'HandManipulateBlockTouchSensors-v0', 'HandManipulateBlockTouchSensors-v1', 'HandManipulateBlockTouchSensorsDense-v0', 'HandManipulateBlockTouchSensorsDense-v1', 'HandManipulateEgg-v0', 'HandManipulateEggDense-v0', 'HandManipulateEggFull-v0', 'HandManipulateEggFullDense-v0', 'HandManipulateEggRotate-v0', 'HandManipulateEggRotateDense-v0', 'HandManipulateEggRotateTouchSensors-v0', 'HandManipulateEggRotateTouchSensors-v1', 'HandManipulateEggRotateTouchSensorsDense-v0', 'HandManipulateEggRotateTouchSensorsDense-v1', 'HandManipulateEggTouchSensors-v0', 'HandManipulateEggTouchSensors-v1', 'HandManipulateEggTouchSensorsDense-v0', 'HandManipulateEggTouchSensorsDense-v1', 'HandManipulatePen-v0', 'HandManipulatePenDense-v0', 'HandManipulatePenFull-v0', 'HandManipulatePenFullDense-v0', 'HandManipulatePenRotate-v0', 'HandManipulatePenRotateDense-v0', 'HandManipulatePenRotateTouchSensors-v0', 'HandManipulatePenRotateTouchSensors-v1', 'HandManipulatePenRotateTouchSensorsDense-v0', 'HandManipulatePenRotateTouchSensorsDense-v1', 'HandManipulatePenTouchSensors-v0', 'HandManipulatePenTouchSensors-v1', 'HandManipulatePenTouchSensorsDense-v0', 'HandManipulatePenTouchSensorsDense-v1', 'HandReach-v0', 'HandReachDense-v0', 'Hopper-v2', 'Hopper-v3', 'Humanoid-v2', 'Humanoid-v3', 'HumanoidStandup-v2', 'InvertedDoublePendulum-v2', 'InvertedPendulum-v2', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'MemorizeDigits-v0', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Pusher-v2', 'Reacher-v2', 'Striker-v2', 'Swimmer-v2', 'Swimmer-v3', 'Taxi-v3', 'Thrower-v2', 'Walker2d-v2', 'Walker2d-v3']\n"
     ]
    }
   ],
   "source": [
    "from gym import envs\n",
    "all_envs = envs.registry.all()\n",
    "env_ids = [env_spec.id for env_spec in all_envs]\n",
    "print(sorted(env_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "sample = env.observation_space.sample()\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    print(action)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(reward)\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the algorithm.\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"LunarLander-v2\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 4,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"torch\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [128, 128, 128, 128],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": True,\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create our RLlib Trainer.\n",
    "trainer = PPOTrainer(config=config)\n",
    "\n",
    "# Run it for n training iterations. A training iteration includes\n",
    "# parallel sample collection by the environment workers as well as\n",
    "# loss calculation on the collected batch and a model update.\n",
    "for _ in range(10):\n",
    "    print(trainer.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained Trainer (and render each timestep to the shell's\n",
    "# output).\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "# How many time steps to run the experiment for.\n",
    "time_steps_total = 1_000_000\n",
    "\n",
    "# Where the trained agents and the logs will end up.\n",
    "local_dir = \"tune_runs_lunarlander\"\n",
    "\n",
    "# Run the experiment.\n",
    "results = tune.run(\n",
    "    agents.ppo.PPOTrainer,\n",
    "    config=config,\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    stop={\"timesteps_total\": time_steps_total},\n",
    "    checkpoint_at_end=True,\n",
    "    checkpoint_freq=10,\n",
    "    # keep_checkpoints_num=10,\n",
    "    local_dir=local_dir,\n",
    "    #restore=restore_checkpoint\n",
    "    #        name=\"PPOTrainer_2022-03-23_15-47-45\",\n",
    "    #resume=AUTO\n",
    ")\n",
    "\n",
    "# Get the checkpoints.\n",
    "checkpoints = results.get_trial_checkpoints_paths(\n",
    "    trial=results.get_best_trial(\"episode_reward_mean\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "for checkpoint in checkpoints:\n",
    "    checkpoint_path = checkpoint[0]\n",
    "    print(\"Checkpoint path:\", checkpoint_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Clean up.\n",
    "subfolders = glob.glob(os.path.join(local_dir, \"*\"), recursive=True)\n",
    "subfolders = [subfolder for subfolder in subfolders if os.path.isdir(subfolder)]\n",
    "for subfolder in subfolders:\n",
    "    subsubfolders = glob.glob(os.path.join(subfolder, \"*\"), recursive=True)\n",
    "    subsubfolders = [subsubfolder for subsubfolder in subsubfolders if os.path.isdir(subsubfolder)]\n",
    "    for subsubfolder in subsubfolders:\n",
    "        subsubfolder_cleaned = subsubfolder.replace(\"[\", \"\").replace(\"]\", \"\").replace(\", \", \"-\")\n",
    "        if subsubfolder_cleaned != subsubfolder:\n",
    "            os.rename(subsubfolder, subsubfolder_cleaned)\n",
    "            print(f\"Renamed {subsubfolder} to {subsubfolder_cleaned}\")\n",
    "\n",
    "# Sound an alarm using unix escape sequence.\n",
    "print(\"Done.\")\n",
    "print(\"\\a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib import agents\n",
    "# Where the trained agents and the logs will end up.\n",
    "local_dir = \"tune_runs_lunarlander\"\n",
    "\n",
    "# Find all the occurences of params.json in the directory tune_runs.\n",
    "params_paths = glob.glob(os.path.join(local_dir, \"**\", \"params.json\"), recursive=True)\n",
    "params_paths = sorted(params_paths)\n",
    "\n",
    "# Find all the checkpoints per params.json.\n",
    "pairs = []\n",
    "for params_path in params_paths:\n",
    "    search_path = os.path.join(os.path.dirname(params_path), \"**\")\n",
    "    checkpoint_paths = glob.glob(search_path, recursive=True)\n",
    "    checkpoint_paths = [checkpoint_path for checkpoint_path in checkpoint_paths if\n",
    "                        not os.path.isdir(checkpoint_path)]\n",
    "    checkpoint_paths = [checkpoint_path for checkpoint_path in checkpoint_paths if\n",
    "                        os.path.basename(checkpoint_path).startswith(\n",
    "                            \"checkpoint-\") and not checkpoint_path.endswith(\".tune_metadata\")]\n",
    "    checkpoint_paths = sorted(checkpoint_paths)\n",
    "    pairs += [(params_path, checkpoint_path) for checkpoint_path in checkpoint_paths]\n",
    "\n",
    "# Get the user input.\n",
    "if len(pairs) == 0:\n",
    "    print(\"No checkpoints found.\")\n",
    "elif len(pairs) == 1:\n",
    "    user_index = 0\n",
    "else:\n",
    "    print(\"Select a checkpoint:\")\n",
    "    for index, (_, subfolder) in enumerate(pairs):\n",
    "        print(f\"{index: >2}: {subfolder}\")\n",
    "    user_index = int(input(\"Enter the index of the checkpoint: \"))\n",
    "\n",
    "# Get config path and checkpoint path.\n",
    "config_path, checkpoint_path = pairs[user_index]\n",
    "assert os.path.exists(config_path)\n",
    "assert os.path.exists(checkpoint_path)\n",
    "print(\"Config path:\", config_path)\n",
    "print(\"Checkpoint path:\", checkpoint_path)\n",
    "\n",
    "# Read the config.\n",
    "with open(config_path, \"r\") as file:\n",
    "    enjoy_config = json.load(file)\n",
    "    enjoy_config = {key: value for key, value in enjoy_config.items() if key not in [\"num_gpus\", \"num_workers\"]}\n",
    "\n",
    "print(enjoy_config, config_path)\n",
    "\n",
    "# Load the agent.\n",
    "print(\"Loading agent...\")\n",
    "agent = agents.ppo.PPOTrainer(config=enjoy_config)\n",
    "agent.restore(checkpoint_path)\n",
    "print(\"Agent loaded.\")\n",
    "\n",
    "# Create the environment.\n",
    "print(\"Creating the environment.\")\n",
    "environment = gym.make('LunarLander-v2')\n",
    "observation = environment.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    #print(observation[\"carriers_next_station_distance\"])\n",
    "    action = agent.compute_action(observation)\n",
    "        \n",
    "    # Execute the action.\n",
    "    observation, reward, done, info = environment.step(action)\n",
    "    \n",
    "    # Render the environment and print the data.\n",
    "    environment.render()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc54ac70318b073a5d84a6318a7bb838fc2ca2d0633c1f59c5496de699c8dccd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('RL_LunarLander')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
